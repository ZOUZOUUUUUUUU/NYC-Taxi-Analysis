import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object ConvertToCSV {
  def main(args: Array[String]): Unit = {
    
    // Cr√©er une session Spark
    val spark = SparkSession.builder()
      .appName("Convert to CSV for Visualization")
      .master("local[*]")
      .config("spark.driver.memory", "4g")
      .getOrCreate()

    import spark.implicits._

    println("================================================================================")
    println("CONVERSION DES R√âSULTATS PARQUET EN CSV")
    println("================================================================================")

    val outputBase = "output"
    val csvBase = "output/csv_for_viz"

    try {
      // 1. Distribution horaire (Phase 3)
      println("\nüìä 1. Conversion : Distribution horaire...")
      val hourlyDist = spark.read.parquet(s"$outputBase/phase3_hourly_distribution")
        .orderBy("hour")
      
      hourlyDist.coalesce(1)
        .write
        .mode("overwrite")
        .option("header", "true")
        .csv(s"$csvBase/hourly_distribution")
      println("‚úÖ Distribution horaire ‚Üí CSV")

      // 2. Top zones de d√©part (Phase 3)
      println("\nüìä 2. Conversion : Top zones de d√©part...")
      val topPickup = spark.read.parquet(s"$outputBase/phase3_top_pickup_zones")
        .orderBy(desc("nombre_departs"))
        .limit(15)
      
      topPickup.coalesce(1)
        .write
        .mode("overwrite")
        .option("header", "true")
        .csv(s"$csvBase/top_pickup_zones")
      println("‚úÖ Top zones d√©part ‚Üí CSV")

      // 3. Modes de paiement (Phase 4)
      println("\nüìä 3. Conversion : Modes de paiement...")
      val paymentDist = spark.read.parquet(s"$outputBase/phase4_payment_distribution")
        .orderBy(desc("nombre_trajets"))
      
      paymentDist.coalesce(1)
        .write
        .mode("overwrite")
        .option("header", "true")
        .csv(s"$csvBase/payment_distribution")
      println("‚úÖ Modes de paiement ‚Üí CSV")

      // 4. √âvolution paiements par date (Phase 4)
      println("\nüìä 4. Conversion : √âvolution paiements...")
      val paymentByDate = spark.read.parquet(s"$outputBase/phase4_payment_by_date")
        .filter(col("trip_date") >= "2024-01-01" && col("trip_date") <= "2024-01-31")
        .orderBy("trip_date")
      
      paymentByDate.coalesce(1)
        .write
        .mode("overwrite")
        .option("header", "true")
        .csv(s"$csvBase/payment_by_date")
      println("‚úÖ √âvolution paiements ‚Üí CSV")

      // 5. Opportunit√©s covoiturage par heure (Phase 5)
      println("\nüìä 5. Conversion : Opportunit√©s covoiturage horaire...")
      val ridesharingHourly = spark.read.parquet(s"$outputBase/phase5_hourly_opportunities")
        .orderBy("hour")
      
      ridesharingHourly.coalesce(1)
        .write
        .mode("overwrite")
        .option("header", "true")
        .csv(s"$csvBase/ridesharing_hourly")
      println("‚úÖ Opportunit√©s covoiturage ‚Üí CSV")

      // 6. R√©sum√© √©conomies (Phase 5)
      println("\nüìä 6. Conversion : R√©sum√© √©conomies...")
      val savingsSummary = spark.read.parquet(s"$outputBase/phase5_savings_analysis")
        .orderBy(desc("economie_argent"))
        .limit(20)
      
      savingsSummary.coalesce(1)
        .write
        .mode("overwrite")
        .option("header", "true")
        .csv(s"$csvBase/savings_summary")
      println("‚úÖ R√©sum√© √©conomies ‚Üí CSV")

      // 7. √âchantillon de donn√©es nettoy√©es
      println("\nüìä 7. Conversion : √âchantillon donn√©es...")
      val sampleData = spark.read.parquet(s"$outputBase/cleaned_taxi_data")
        .sample(0.001) // 0.1% des donn√©es
        .select(
          "trip_distance_km", "trip_duration_minutes", "average_speed_kmh",
          "hour", "day_name", "fare_amount", "tip_percentage", "payment_type_label",
          "trip_category", "time_period"
        )
      
      sampleData.coalesce(1)
        .write
        .mode("overwrite")
        .option("header", "true")
        .csv(s"$csvBase/sample_data")
      println("‚úÖ √âchantillon donn√©es ‚Üí CSV")

      println("\n" + "=" * 80)
      println("‚úÖ TOUTES LES CONVERSIONS TERMIN√âES AVEC SUCC√àS")
      println("=" * 80)
      println(s"üìÅ Fichiers CSV disponibles dans : $csvBase/")
      println("\nVous pouvez maintenant ex√©cuter le script Python de visualisation.")

    } catch {
      case e: Exception =>
        println(s"\n‚ùå ERREUR lors de la conversion : ${e.getMessage}")
        e.printStackTrace()
    } finally {
      spark.stop()
    }
  }
}
